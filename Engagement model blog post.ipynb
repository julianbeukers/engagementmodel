{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io import gbq\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.getLogger('googleapiclient.discovery_cache').setLevel(logging.ERROR)\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from apiclient.http import MediaFileUpload\n",
    "from apiclient.discovery import build\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create a hitid custom dimension via Google Tag Manager - hitscope\n",
    "# create a custom metric in Google Analytics - hit scope\n",
    "# import data from Google Big Query\n",
    "# score users who visit page y after page x\n",
    "# create a Google Analytics import file\n",
    "# import engagement scores to Google Analytics via Google Analytics Management API\n",
    "# more info on: www.......com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Change the following variables in the query:\n",
    "- project id\n",
    "- table\n",
    "- start and end of the timestamp\n",
    "- page regex in the where statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data GBQ, sessions on page x (STEP 1 in funnel)\n",
    "conf = {\n",
    "   'project_id': 'YOUR PROJECT ID',\n",
    "   'table': 'YOUR GOOGLE BIGQUERY TABLE'\n",
    "}\n",
    "q = \"\"\"\n",
    " SELECT\n",
    "    CONCAT(fullVisitorId,STRING(visitId)) AS sessionId,\n",
    "    hits.page.pagePath AS page,\n",
    "    hits.hitNumber AS hitnumber\n",
    "  FROM (TABLE_DATE_RANGE([123780675.ga_sessions_], TIMESTAMP('2018-11-02'), TIMESTAMP('2018-11-04')))\n",
    "  WHERE\n",
    "    REGEXP_MATCH(hits.page.pagePath,'FILL IN FIRST STEP IN FUNNEL')\n",
    "  \"\"\"\n",
    "df_pageX = gbq.read_gbq(q, project_id=conf['project_id'], dialect='legacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Change the following variables in the query:\n",
    "- project id\n",
    "- table\n",
    "- start and end of the timestamp\n",
    "- page regex in the where statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data GBQ, sessions on page y (STEP 2 in funnel)\n",
    "conf = {\n",
    "   'project_id': 'YOUR PROJECT ID',\n",
    "   'table': 'YOUR GOOGLE BIGQUERY TABLE'\n",
    "}\n",
    "q = \"\"\"\n",
    " SELECT\n",
    "    CONCAT(fullVisitorId,STRING(visitId)) AS sessionId,\n",
    "    hits.page.pagePath AS page,\n",
    "    hits.hitNumber AS hitnumber,\n",
    "  FROM (TABLE_DATE_RANGE([123780675.ga_sessions_], TIMESTAMP('2018-11-02'), TIMESTAMP('2018-11-04')))\n",
    "  WHERE\n",
    "    REGEXP_MATCH(hits.page.pagePath,'FILL IN SECOND STEP IN FUNNEL')\n",
    "  \"\"\"\n",
    "df_pageY = gbq.read_gbq(q, project_id=conf['project_id'], dialect='legacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two dataframes df_facilities & df_map\n",
    "df = pd.merge(df_pageX, df_pageY,  how='left', left_on='sessionId', right_on = 'sessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the sessions with first page x followed by page y\n",
    "df = df.drop(df[df.hitnumber_x > df.hitnumber_y].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop rows with duplicates, drop if session visit funnel multiple times\n",
    "df = df.drop_duplicates(subset=['sessionId', 'page_x', 'page_y'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create score variables per step in funnel \n",
    "score1 = 1\n",
    "score2 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column and assign variable score1 to all sessions with page x\n",
    "df['score1'] = score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign score 2 to sessions with page y, if not give score value 0\n",
    "df['score2'] = np.where(df['page_y'].notnull(), score2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sessionId       object\n",
       "page_x          object\n",
       "hitnumber_x      int64\n",
       "page_y          object\n",
       "hitnumber_y    float64\n",
       "score1           int64\n",
       "score2           int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_count(grp):\n",
    "             grp['count_row'] = grp['sessionId'].count()\n",
    "             return grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a new column which check if their are sessions who have multiple hits on page y\n",
    "df = df.groupby(['sessionId']).apply(cust_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# divide the sessions with score 2 by the count of rows per session\n",
    "df['score2'] = df['score2'] / df['count_row']\n",
    "df['score2'] = df['score2'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the total score\n",
    "def total_score(score1, score2):\n",
    "    total = score1 + score2\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum total sore\n",
    "df['total_score'] = total_score(df['score1'], df['score2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate all total scores per sessionId + create new column 'ga:metric5'\n",
    "def cust_sum(grp):\n",
    "             grp['ga:metric5'] = grp['total_score'].sum()\n",
    "             return grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by sessionId and sum the total score\n",
    "df_import = df.groupby(['sessionId']).apply(cust_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will need a int to import, check your settings of the custom metric\n",
    "# Change index 5 into your own index number\n",
    "df_import['ga:metric5'] = df_import['ga:metric5'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates, keep one unique sessionId\n",
    "df_import = df_import.drop_duplicates(['sessionId'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add last hitID of each session to df_import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data GBQ, all sessionId's with dimension hitId. Change index=63 in to your own index number.\n",
    "conf = {\n",
    "   'project_id': 'YOUR PROJECT ID',\n",
    "   'table': 'YOUR GOOGLE BIGQUERY TABLE'\n",
    "}\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "  CONCAT(fullVisitorId,STRING(visitId)) AS sessionId,\n",
    "  hits.hitNumber as hitnumber,\n",
    "  hits.customDimensions.value AS cd63\n",
    "FROM (TABLE_DATE_RANGE([123780675.ga_sessions_], TIMESTAMP('2018-11-02'), TIMESTAMP('2018-11-04')))\n",
    "WHERE\n",
    "  hits.customDimensions.index=63\n",
    "  AND hits.type = 'PAGE'\n",
    "  \"\"\"\n",
    "df_hitId = gbq.read_gbq(q, project_id=conf['project_id'], dialect='legacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column name to 'ga:dimension63'. Change index=63 into your own index number.\n",
    "df_hitId = df_hitId.rename(columns={'cd63': 'ga:dimension63'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hitId.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we need one hitId of each session, lets keep only the last hitId\n",
    "df_hitId = df_hitId.drop_duplicates(subset=['sessionId'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two dataframes to add a column with the last and correct hitID\n",
    "df_import = pd.merge(df_import, df_hitId,  how='left', left_on='sessionId', right_on= 'sessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to keep/drop columns\n",
    "def keep_cols(DataFrame, keep_these):\n",
    "    \"\"\"Keep only the columns [keep_these] in a DataFrame, delete\n",
    "    all other columns. \n",
    "    \"\"\"\n",
    "    drop_these = list(set(list(DataFrame)) - set(keep_these))\n",
    "    return DataFrame.drop(drop_these, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need all these columns, so keep only the columns which are necessary for the import. \n",
    "# Change index 5 into your own index number.\n",
    "df_import = df_import.pipe(keep_cols, ['ga:dimension63', 'ga:metric5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if needed, change order of the columns to prepare for import\n",
    "df_import = df_import[['ga:dimension63', 'ga:metric5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check what is in our csv\n",
    "df_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 63 into your own index number.\n",
    "df_import = df_import.dropna(subset=['ga:dimension63'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ga:dimension63    object\n",
       "ga:metric5         int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_import.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose number of chunks you need, this is depending on the number of rows. Limit per chunk = 1000\n",
    "number_of_chunks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows in chunk: 1485.8333333333333 rows\n"
     ]
    }
   ],
   "source": [
    "rows_chunk = len(df_import) / number_of_chunks\n",
    "print(\"number of rows in chunk: {} rows\".format(rows_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the csv file in to the number of chunks you need\n",
    "for i, df_import in enumerate(np.array_split(df_import,number_of_chunks)):\n",
    "    with open(f\"out{i}.csv\",\"w\") as fo:\n",
    "            fo.write(df_import.to_csv(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max rows per API request 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches for import\n",
    "csv_filename1 = 'out0.csv'\n",
    "csv_filename2 = 'out1.csv'\n",
    "csv_filename3 = 'out2.csv'\n",
    "csv_filename4 = 'out3.csv'\n",
    "csv_filename5 = 'out4.csv'\n",
    "csv_filename6 = 'out5.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change the variables into yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_service(api_name, api_version, scopes, key_file_location):\n",
    "    \"\"\"Get a service that communicates to a Google API.\n",
    "\n",
    "    Args:\n",
    "    api_name: The name of the api to connect to.\n",
    "    api_version: The api version to connect to.\n",
    "    scope: A list auth scopes to authorize for the application.\n",
    "    key_file_location: The path to a valid service account JSON key file.\n",
    "\n",
    "    Returns:\n",
    "    A service that is connected to the specified API.\n",
    "    \"\"\"\n",
    "\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        key_file_location, scopes=scopes)\n",
    "\n",
    "    # Build the service object.\n",
    "    service = build(api_name, api_version, credentials=credentials)\n",
    "\n",
    "    return service\n",
    "\n",
    "scopes = ['https://www.googleapis.com/auth/analytics.edit',\n",
    "          'https://www.googleapis.com/auth/analytics']\n",
    "\n",
    "# fill in your own variables.\n",
    "key_file_location = 'credentials/dataimport.json' # example file path\n",
    "account_id = '6XXXXXXX' # example, fill in Google Analytics account id\n",
    "web_property_id = 'UA-6XXXXXXX-1' # fill in UA tracking id\n",
    "custom_data_source_id = \"DN-xz23DR6as3sMml1jRdw\" # example, fill in import id\n",
    "analytics = get_service('analytics', 'v3', scopes, key_file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the different batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# batch 1\n",
    "try:\n",
    "    media = MediaFileUpload(csv_filename1,\n",
    "                          mimetype='application/octet-stream',\n",
    "                          resumable=False)\n",
    "    daily_upload = analytics.management().uploads().uploadData(\n",
    "        accountId=account_id,\n",
    "        webPropertyId=web_property_id,\n",
    "        customDataSourceId=custom_data_source_id,\n",
    "        media_body=media).execute()\n",
    "    \n",
    "    print(\"Upload successful!\")\n",
    "\n",
    "except TypeError as error:\n",
    "    # Handle errors in constructing a query.\n",
    "    print('There was an error in constructing your query : %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# batch 2\n",
    "time.sleep(3)\n",
    "try:\n",
    "    media = MediaFileUpload(csv_filename2,\n",
    "                          mimetype='application/octet-stream',\n",
    "                          resumable=False)\n",
    "    daily_upload = analytics.management().uploads().uploadData(\n",
    "        accountId=account_id,\n",
    "        webPropertyId=web_property_id,\n",
    "        customDataSourceId=custom_data_source_id,\n",
    "        media_body=media).execute()\n",
    "    \n",
    "    print(\"Upload successful!\")\n",
    "\n",
    "except TypeError as error:\n",
    "    # Handle errors in constructing a query.\n",
    "    print('There was an error in constructing your query : %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# batch 3\n",
    "time.sleep(3)\n",
    "try:\n",
    "    media = MediaFileUpload(csv_filename3,\n",
    "                          mimetype='application/octet-stream',\n",
    "                          resumable=False)\n",
    "    daily_upload = analytics.management().uploads().uploadData(\n",
    "        accountId=account_id,\n",
    "        webPropertyId=web_property_id,\n",
    "        customDataSourceId=custom_data_source_id,\n",
    "        media_body=media).execute()\n",
    "    \n",
    "    print(\"Upload successful!\")\n",
    "\n",
    "except TypeError as error:\n",
    "    # Handle errors in constructing a query.\n",
    "    print('There was an error in constructing your query : %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# batch 4\n",
    "time.sleep(3)\n",
    "try:\n",
    "    media = MediaFileUpload(csv_filename4,\n",
    "                          mimetype='application/octet-stream',\n",
    "                          resumable=False)\n",
    "    daily_upload = analytics.management().uploads().uploadData(\n",
    "        accountId=account_id,\n",
    "        webPropertyId=web_property_id,\n",
    "        customDataSourceId=custom_data_source_id,\n",
    "        media_body=media).execute()\n",
    "    \n",
    "    print(\"Upload successful!\")\n",
    "\n",
    "except TypeError as error:\n",
    "    # Handle errors in constructing a query.\n",
    "    print('There was an error in constructing your query : %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# batch 5\n",
    "time.sleep(3)\n",
    "try:\n",
    "    media = MediaFileUpload(csv_filename5,\n",
    "                          mimetype='application/octet-stream',\n",
    "                          resumable=False)\n",
    "    daily_upload = analytics.management().uploads().uploadData(\n",
    "        accountId=account_id,\n",
    "        webPropertyId=web_property_id,\n",
    "        customDataSourceId=custom_data_source_id,\n",
    "        media_body=media).execute()\n",
    "    \n",
    "    print(\"Upload successful!\")\n",
    "\n",
    "except TypeError as error:\n",
    "    # Handle errors in constructing a query.\n",
    "    print('There was an error in constructing your query : %s' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful!\n"
     ]
    }
   ],
   "source": [
    "# batch 6\n",
    "time.sleep(3)\n",
    "try:\n",
    "    media = MediaFileUpload(csv_filename6,\n",
    "                          mimetype='application/octet-stream',\n",
    "                          resumable=False)\n",
    "    daily_upload = analytics.management().uploads().uploadData(\n",
    "        accountId=account_id,\n",
    "        webPropertyId=web_property_id,\n",
    "        customDataSourceId=custom_data_source_id,\n",
    "        media_body=media).execute()\n",
    "    \n",
    "    print(\"Upload successful!\")\n",
    "\n",
    "except TypeError as error:\n",
    "    # Handle errors in constructing a query.\n",
    "    print('There was an error in constructing your query : %s' % error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
